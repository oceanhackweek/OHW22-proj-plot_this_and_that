{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0872b1fb-b315-488d-83b1-8a578d931760",
   "metadata": {},
   "source": [
    "## Script for getting file names from Thredds server to import netCDF data\n",
    "### Examples:\n",
    "```getLTSPfileName(\"GBRLSL\", \"velocity\")```\n",
    "### Returns all velocity data from site: GBRLSL\n",
    "\n",
    "\n",
    "```getLTSPfileName(\"GBRMYR\", \"velocity-hourly\")```\n",
    "### Returns the velocity hourly LTSP filename from site: GBRMYR\n",
    "\n",
    "#### Currently built for importing IMOS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de71143-0db1-4d0c-ad19-b0b7f53966a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 10000)\n",
    "\n",
    "def args():\n",
    "    parser = argparse.ArgumentParser(description=\"Get LTSP file name\")\n",
    "    parser.add_argument('-site', dest='site', help='site code, like NRMMAI',  type=str, default=None, required=True)\n",
    "    parser.add_argument('-product',dest='product', help='product type: aggregated, hourly, velocity-hourly or gridded', type=str, default='hourly', required=True)\n",
    "    parser.add_argument('-QC',dest='QC', help='for the hourly, QCed data only. Default True', type=bool, default=True, required=False)\n",
    "    parser.add_argument('-param',dest='param', help='for the aggregated, parameter, like TEMP, or \"velocity\"', type=str, default='TEMP', required=False)\n",
    "    parser.add_argument('-weburl',dest='webURL', help='url root for the file: S3: Amazon AWS (for download, fastest), wget (AODN THREDDS, for download), opendap (AODN THREDDS to open remotely). Default opendap', type=str, default='opendap', required=False)\n",
    "\n",
    "    vargs = parser.parse_args()\n",
    "    return(vargs)\n",
    "\n",
    "\n",
    "def getLTSPfileName(site, product=\"gridded\", QC=True, param=\"TEMP\", webURL=\"opendap\"):\n",
    "    '''\n",
    "    get the url of the LTSP files\n",
    "    \n",
    "    require: pandas\n",
    "    site: the site_code\n",
    "    product: product type )aggregated, hourly or gridded)\n",
    "    QC: for the hourly, include only good data (default True)\n",
    "    param: for aggregated product, parameter code as IMOS standard (e.g. TEMP)\n",
    "    webURL: web source of the file (S3: Amazon AWS (fastest), wget (AODN THREDDS, to download),\n",
    "            opendap (AODN THREDDS to open remotely)\n",
    "    E. Klein. eklein at ocean-analytics dot com dot au\n",
    "    Adjusted by T. Armstrong - AIMS\n",
    "    '''\n",
    "    \n",
    "    if webURL == \"opendap\": \n",
    "        WEBROOT = 'http://thredds.aodn.org.au/thredds/dodsC/'\n",
    "    elif webURL == \"wget\":\n",
    "        WEBROOT = 'http://thredds.aodn.org.au/thredds/fileServer/'\n",
    "    elif webURL == \"S3\":\n",
    "        WEBROOT = 'https://s3-ap-southeast-2.amazon.com/imos-data/'\n",
    "    else:\n",
    "        print(\"ERROR: wrong webURL: it must be one of S3, opendap or wget\")\n",
    "\n",
    "  \n",
    "    urlGeoServer = \"http://geoserver-123.aodn.org.au/geoserver/ows?typeName=moorings_all_map&SERVICE=WFS&REQUEST=GetFeature&VERSION=1.0.0&outputFormat=csv&CQL_FILTER=(realtime='FALSE')and(site_code='\" + site + \"')\"\n",
    "    df = pd.read_csv(urlGeoServer)\n",
    "    url = df['url']\n",
    "    \n",
    "    #fileName = df$url[grepl(paste0(product,\"-timeseries\"), df$url)]\n",
    "    fileName = \"TEST\"\n",
    "    \n",
    "    #Get long time series products (LTSP)\n",
    "    if product == \"gridded\": \n",
    "        fileName = url[url.str.contains(\"gridded\")]\n",
    "    elif product==\"velocity-hourly\":\n",
    "        fileName = url[url.str.contains(\"velocity-hourly\")]\n",
    "        # print(WEBROOT + fileName)\n",
    "    elif product==\"hourly\":\n",
    "        if QC:\n",
    "            fileName = url[url.str.contains(\"(?<!velocity-)hourly-timeseries(?!-including)\", regex=True)]\n",
    "        else:\n",
    "            fileName = url[url.str.contains(\"including-non\")]\n",
    "    elif product==\"aggregated\":\n",
    "        fileName = url[url.str.contains(param) & url.str.contains(\"aggregated\")]\n",
    "        \n",
    "    # Get list of temp/vel files\n",
    "    elif product==\"temperature\":\n",
    "        fileName = url[url.str.contains(site + \"/Temperature\") & url.str.contains(\"FV01\")]\n",
    "        return WEBROOT + fileName\n",
    "    elif product==\"velocity\":\n",
    "        fileName = url[url.str.contains(site + \"/Velocity\") & url.str.contains(\"FV01\")]\n",
    "        return WEBROOT + fileName\n",
    "    # Error\n",
    "    else:\n",
    "        print(\"ERROR: invalid combination of arguments or wrong names\")\n",
    "    return WEBROOT + fileName.to_string(index=False, header=False).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be83e33-5d58-4de0-b488-102835a472a4",
   "metadata": {},
   "source": [
    "### Get all filenames for sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fd2a564-9663-4784-b9e9-488472c0ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all filenames for sites moorings\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "import xarray as xr\n",
    "sites = ['GBRLSL', 'GBRLSH', 'GBRMYR', 'GBRPPS', 'GBRHIS', 'GBROTE', 'GBRCCH', 'GBRELR', 'GBRHIN', 'NWSROW', 'NWSLYN', 'NWSBAR', 'NWSBRW', 'TAN100']\n",
    "# sites = ['GBRLSL', 'GBRLSH']\n",
    "vel_hourly = []\n",
    "temp_hourly = []\n",
    "for my_site in sites:\n",
    "    # getLTSPfileName(my_site, \"velocity-hourly\")\n",
    "    vel_hourly.append(getLTSPfileName(my_site, \"velocity-hourly\"))\n",
    "    temp_hourly.append(getLTSPfileName(my_site, \"hourly\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e055c5-dedd-4e4b-bbb3-a118507fa320",
   "metadata": {},
   "source": [
    "### Download all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bccf9165-4ce0-44b1-a170-49ab7f14b7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in vel_hourly:\n",
    "    counter = 0\n",
    "    file = xr.open_dataset(file).load()\n",
    "    file.to_netcdf(path = f\"C:\\\\Users\\\\tarmstro\\\\Desktop\\\\Scripting\\\\{sites[counter]}_vel_hourly.nc\")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce56a63d-bdc7-4c84-8707-82c9164ce00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = xr.open_dataset(vel_hourly[0]).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77319d40-3e96-4901-a215-3371de927674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855f2711-3170-48fa-aacc-f54a6664a980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
